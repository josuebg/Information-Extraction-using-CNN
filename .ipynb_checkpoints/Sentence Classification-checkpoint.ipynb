{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xenon\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Dropout, concatenate\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "#from tensorflow.keras.utils import model_to_do\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(raw_predictions, label_encoder):\n",
    "    #convert raw predicition to class indexes\n",
    "    threshold = 0.5\n",
    "    class_predictions = [(x > threshold).astype(int) for x in model.predict(x_test)]\n",
    "    \n",
    "    #convert raw predicitions to class indexes\n",
    "    threshold = 0.5\n",
    "    class_predicitions = [(x > threshold).astype(int) for x in model.predict(x_test)]\n",
    "    \n",
    "    # select only one class\n",
    "    # i.e., the dim in the vector with 1.0 all other are at 0.0\n",
    "    class_index = ([np.argmax(x) for x in class_predictions])\n",
    "    \n",
    "    #convert back to original class names\n",
    "    pred_classes = label_encoder.inverse_transform(class_index)\n",
    "    \n",
    "    #print precision, recall, f1-score report\n",
    "    print(classification_report(y_test, pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_embeddings():\n",
    "    glove_dir = './Datasets/glove.6B'\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix(embedding_index, vocabulary, embedding_dim = 100):\n",
    "    embeddings_matrix = np.random.rand(len(vocabulary)+1, embedding_dim)\n",
    "    \n",
    "    for i, word in enumerate(vocabulary):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "    print(\"Matrix shape : {}\" .format(embeddings_matrix.shape))\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_layer(embeddings_matrix, name, max_len, trainable=False):\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=embeddings_matrix.shape[0],\n",
    "        output_dim=embeddings_matrix.shape[1],\n",
    "        input_length=max_len,\n",
    "        weights=[embeddings_matrix],\n",
    "        trainable=trainable,\n",
    "        name=name)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_pool(x_input, max_len, sufix, n_grams=[3,4,5], feature_maps=100):\n",
    "    branches = []\n",
    "    for n in n_grams:\n",
    "        branch = Conv1D(filters=feature_maps, kernel_size=n, activation=relu, name='Conv_'+sufix+'_'+str(n))(x_input)\n",
    "        branch = MaxPooling1D(pool_size=max_len-n+1, strides=None, padding='valid', name='MaxPooling_'+sufix+'_'+str(n))(branch)\n",
    "        branch = Flatten(name='Flatten_'+sufix+'_'+str(n))(branch)\n",
    "        branches.append(branch)\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_rand(embedding_dim=100, vocab_size=1000, max_len=50):\n",
    "    # create the embedding layer\n",
    "    embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "    embedding_layer = get_embeddings_layer(embedding_matrix, 'embedding_layer_dynamic', max_len, trainable=True)\n",
    "\n",
    "    # connect the input with the embedding layer\n",
    "    i = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    x = embedding_layer(i)\n",
    "\n",
    "    # generate several branches in the network, each for a different convolution+pooling operation,\n",
    "    # and concatenate the result of each branch into a single vector\n",
    "    branches = get_conv_pool(x, max_len, 'dynamic')\n",
    "    z = concatenate(branches, axis=-1)\n",
    "    z = Dropout(0.5)(z)\n",
    "\n",
    "    # pass the concatenated vector to the predition layer\n",
    "    o = Dense(1, activation='sigmoid', name='output')(z)\n",
    "\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Xenon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Xenon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_1 = get_cnn_rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-170f47caa9b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "SVG(model_to_dot(model_1, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_fasttext_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape : (400001, 100)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = embeddings_index.keys()\n",
    "# replace this by the vocabulary of the dataset you want to train\n",
    "embeddings_matrix = create_embeddings_matrix(embeddings_index, vocabulary, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = get_embeddings_layer(embeddings_matrix, 'embedding_layer_static', 55, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_pre_trained_embeddings(embedding_layer, max_len):\n",
    "    # connect the input with the embedding layer\n",
    "    i = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    x = embedding_layer(i)\n",
    "\n",
    "    # generate several branches in the network, each for a different convolution+pooling operation,\n",
    "    # and concatenate the result of each branch into a single vector\n",
    "    branches = get_conv_pool(x, max_len, 'static')\n",
    "    z = concatenate(branches, axis=-1)\n",
    "\n",
    "    # pass the concatenated vector to the predition layer\n",
    "    o = Dense(1, activation='sigmoid', name='output')(z)\n",
    "\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_pre_trained_embeddings(embedding_layer, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words vector are fined tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = get_embeddings_layer(embeddings_matrix, 'embedding_layer_dynamic', 55, trainable=True)\n",
    "model = get_cnn_pre_trained_embeddings(embedding_layer, 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Multichannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_channel_1 = get_embeddings_layer(embeddings_matrix, 'embedding_layer_dynamic', 55, trainable=True)\n",
    "embedding_layer_channel_2 = get_embeddings_layer(embeddings_matrix, 'embedding_layer_static', 55, trainable=False)\n",
    "\n",
    "def get_cnn_multichannel(embedding_layer_channel_1, embedding_layer_channel_2, max_len):\n",
    "\n",
    "    # dynamic channel\n",
    "    input_dynamic = Input(shape=(max_len,), dtype='int32', name='input_dynamic')\n",
    "    x = embedding_layer_channel_1(input_dynamic)\n",
    "    branches_dynamic = get_conv_pool(x, max_len, 'static')\n",
    "    z_dynamic = concatenate(branches_dynamic, axis=-1)\n",
    "\n",
    "    # static channel\n",
    "    input_static = Input(shape=(max_len,), dtype='int32', name='input_static')\n",
    "    x = embedding_layer_channel_2(input_static)\n",
    "    branches_static = get_conv_pool(x, max_len, 'dynamic')\n",
    "    z_static = concatenate(branches_static, axis=-1)\n",
    "\n",
    "    # concatenate both models and pass to classification layer\n",
    "    z = concatenate([z_static,z_dynamic], axis=-1)\n",
    "\n",
    "    # pass the concatenated vector to the predition layer\n",
    "    o = Dense(6, activation='sigmoid', name='output')(z)\n",
    "\n",
    "    model = Model(inputs=[input_static, input_dynamic], outputs=o)\n",
    "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_multichannel(embedding_layer_channel_1, embedding_layer_channel_2, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
